{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.1+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.20.1+cpu)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.46.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\priyamvadha pradeep\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\priyamvadha pradeep\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\priyamvadha pradeep\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Priyamvadha Pradeep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, text_dim, img_channels, img_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.text_embed = nn.Linear(text_dim, noise_dim)  # Embed text features\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim * 2, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, img_size * img_size * img_channels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, text_features):\n",
    "        text_embedded = self.text_embed(text_features)\n",
    "        combined_input = torch.cat([noise, text_embedded], dim=1)\n",
    "        img = self.model(combined_input).view(-1, 3, 64, 64)  # Assumes 64x64 images, 3 channels\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, img_size, text_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_embed = nn.Linear(img_size * img_size * img_channels, 256)\n",
    "        self.text_embed = nn.Linear(text_dim, 256)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, text_features):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        img_embedded = self.img_embed(img_flat)\n",
    "        text_embedded = self.text_embed(text_features)\n",
    "        combined_input = torch.cat([img_embedded, text_embedded], dim=1)\n",
    "        validity = self.model(combined_input)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "noise_dim = 100\n",
    "text_dim = 384  # Using all-MiniLM-L6-v2, which outputs embeddings of size 384\n",
    "img_channels = 3\n",
    "img_size = 64\n",
    "\n",
    "G = Generator(noise_dim, text_dim, img_channels, img_size)\n",
    "D = Discriminator(img_channels, img_size, text_dim)\n",
    "\n",
    "# Define loss and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b63601b2864feeb8197ee166c70c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Priyamvadha Pradeep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Priyamvadha Pradeep\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690c2b281b8f43d295b7ba94b0554e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42b2f3dd98444b5bfdc5d970c8607fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42054f5a78149ab885a0131fdd1cd6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0bd91e4bf142af87898ea85f4a6c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371d1c170af74788ab1035bc729919b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0f3b0accdd404ab12a199c0627765b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b741ef4d464769a058b26e1ed73dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548faf433db64f3ea35d3447383041ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf3ef7f2764460d9658f9c6fd3f5e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fc78de53ba401cb78390db327343bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert text description to embedding\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "text_description = \"a red circle\"  # Your text description\n",
    "text_embedding = torch.tensor(text_model.encode(text_description)).unsqueeze(0)  # Shape: (1, text_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] | D Loss: 1.4676 | G Loss: 0.6919\n",
      "Epoch [2/50] | D Loss: 1.3621 | G Loss: 0.7280\n",
      "Epoch [3/50] | D Loss: 1.2151 | G Loss: 0.8043\n",
      "Epoch [4/50] | D Loss: 0.9959 | G Loss: 1.0950\n",
      "Epoch [5/50] | D Loss: 1.3373 | G Loss: 0.8915\n",
      "Epoch [6/50] | D Loss: 1.5806 | G Loss: 0.9549\n",
      "Epoch [7/50] | D Loss: 1.2623 | G Loss: 0.9554\n",
      "Epoch [8/50] | D Loss: 1.2641 | G Loss: 0.9782\n",
      "Epoch [9/50] | D Loss: 1.8527 | G Loss: 1.5321\n",
      "Epoch [10/50] | D Loss: 1.1707 | G Loss: 1.1492\n",
      "Epoch [11/50] | D Loss: 1.1353 | G Loss: 1.0077\n",
      "Epoch [12/50] | D Loss: 1.2049 | G Loss: 1.8296\n",
      "Epoch [13/50] | D Loss: 0.9696 | G Loss: 1.5436\n",
      "Epoch [14/50] | D Loss: 1.1401 | G Loss: 1.2927\n",
      "Epoch [15/50] | D Loss: 1.3489 | G Loss: 1.5601\n",
      "Epoch [16/50] | D Loss: 1.1667 | G Loss: 1.1085\n",
      "Epoch [17/50] | D Loss: 1.4207 | G Loss: 1.3174\n",
      "Epoch [18/50] | D Loss: 1.8804 | G Loss: 1.5302\n",
      "Epoch [19/50] | D Loss: 1.3367 | G Loss: 1.3369\n",
      "Epoch [20/50] | D Loss: 1.4729 | G Loss: 0.7738\n",
      "Epoch [21/50] | D Loss: 1.4301 | G Loss: 0.8436\n",
      "Epoch [22/50] | D Loss: 1.0193 | G Loss: 1.0115\n",
      "Epoch [23/50] | D Loss: 0.8834 | G Loss: 1.3609\n",
      "Epoch [24/50] | D Loss: 0.9612 | G Loss: 1.1803\n",
      "Epoch [25/50] | D Loss: 1.1078 | G Loss: 1.2360\n",
      "Epoch [26/50] | D Loss: 1.2705 | G Loss: 1.0176\n",
      "Epoch [27/50] | D Loss: 1.4633 | G Loss: 0.9339\n",
      "Epoch [28/50] | D Loss: 1.6101 | G Loss: 1.5367\n",
      "Epoch [29/50] | D Loss: 1.0850 | G Loss: 1.4466\n",
      "Epoch [30/50] | D Loss: 1.6160 | G Loss: 0.8701\n",
      "Epoch [31/50] | D Loss: 1.3312 | G Loss: 0.8749\n",
      "Epoch [32/50] | D Loss: 1.2707 | G Loss: 0.8330\n",
      "Epoch [33/50] | D Loss: 1.3053 | G Loss: 0.8493\n",
      "Epoch [34/50] | D Loss: 1.1522 | G Loss: 0.9187\n",
      "Epoch [35/50] | D Loss: 1.5225 | G Loss: 0.8653\n",
      "Epoch [36/50] | D Loss: 1.1739 | G Loss: 0.8993\n",
      "Epoch [37/50] | D Loss: 0.9388 | G Loss: 1.1597\n",
      "Epoch [38/50] | D Loss: 1.2121 | G Loss: 1.0192\n",
      "Epoch [39/50] | D Loss: 1.3244 | G Loss: 0.9532\n",
      "Epoch [40/50] | D Loss: 1.2262 | G Loss: 1.1167\n",
      "Epoch [41/50] | D Loss: 1.5697 | G Loss: 0.8386\n",
      "Epoch [42/50] | D Loss: 1.5430 | G Loss: 0.9287\n",
      "Epoch [43/50] | D Loss: 1.3128 | G Loss: 0.8290\n",
      "Epoch [44/50] | D Loss: 1.1922 | G Loss: 1.4093\n",
      "Epoch [45/50] | D Loss: 1.7308 | G Loss: 1.1760\n",
      "Epoch [46/50] | D Loss: 1.3666 | G Loss: 0.8434\n",
      "Epoch [47/50] | D Loss: 1.8068 | G Loss: 0.8835\n",
      "Epoch [48/50] | D Loss: 1.1479 | G Loss: 0.9868\n",
      "Epoch [49/50] | D Loss: 1.5717 | G Loss: 0.8231\n",
      "Epoch [50/50] | D Loss: 1.2474 | G Loss: 0.8626\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for _ in range(50):  # 50 iterations per epoch\n",
    "        # Train Discriminator\n",
    "        real_imgs = torch.randn(batch_size, img_channels, img_size, img_size)  # Replace with real images if available\n",
    "        noise = torch.randn(batch_size, noise_dim)\n",
    "        fake_imgs = G(noise, text_embedding.repeat(batch_size, 1)).detach()\n",
    "\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        real_loss = criterion(D(real_imgs, text_embedding.repeat(batch_size, 1)), real_labels)\n",
    "        fake_loss = criterion(D(fake_imgs, text_embedding.repeat(batch_size, 1)), fake_labels)\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        noise = torch.randn(batch_size, noise_dim)\n",
    "        fake_imgs = G(noise, text_embedding.repeat(batch_size, 1))\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        g_loss = criterion(D(fake_imgs, text_embedding.repeat(batch_size, 1)), real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    # Save generated images every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        save_image(fake_imgs.data[:16], f\"single_text_generated_epoch_{epoch + 1}.png\", nrow=4, normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
